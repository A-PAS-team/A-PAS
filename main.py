import cv2
import torch
import torch.nn as nn
import numpy as np
from ultralytics import YOLO
from collections import deque

# ==========================================
# âš™ï¸ [ì„¤ì •] í•™ìŠµ í™˜ê²½ê³¼ 100% ì¼ì¹˜í•´ì•¼ í•¨
# ==========================================
# 1. ì˜ìƒ í•´ìƒë„ (í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ ì„¤ì •)
IMG_W, IMG_H = 1920, 1080 

# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° (í•™ìŠµ ë•Œ ìˆ˜ì •í•œ ê°’ê³¼ ë™ì¼í•˜ê²Œ ì„¤ì •)
SEQ_LENGTH = 5     
PRED_LENGTH = 3    
HIDDEN_SIZE = 128
NUM_LAYERS = 2

# 3. ëª¨ë¸ ê²½ë¡œ
YOLO_MODEL_PATH = "yolov8n.pt"
LSTM_MODEL_PATH = "models/best_trajectory_model.pth"
VIDEO_PATH = "my_video.mp4" # ì‹¤ì œ ì˜ìƒ íŒŒì¼ëª…ìœ¼ë¡œ ìˆ˜ì •

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==========================================
# ğŸ§  [ëª¨ë¸ ì •ì˜] í•™ìŠµ ë•Œì™€ ë™ì¼í•œ êµ¬ì¡°
# ==========================================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # batch_first=True ê°€ í•µì‹¬ì…ë‹ˆë‹¤.
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, PRED_LENGTH * 2)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out.view(-1, PRED_LENGTH, 2)

# ==========================================
# ğŸš€ [ì¤€ë¹„] ëª¨ë¸ ë¡œë“œ
# ==========================================
print(f"ğŸš€ ì‹œìŠ¤í…œ ì‹œì‘ (ì¥ì¹˜: {device})")
yolo_model = YOLO(YOLO_MODEL_PATH)
lstm_model = LSTMModel(2, HIDDEN_SIZE, NUM_LAYERS).to(device)
lstm_model.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device))
lstm_model.eval()

track_history = {}
cap = cv2.VideoCapture(VIDEO_PATH)

# ==========================================
# ğŸ¬ [ì‹¤í–‰] ë©”ì¸ ë£¨í”„
# ==========================================
while cap.isOpened():
    success, frame = cap.read()
    if not success: break

    # ì˜ìƒ í¬ê¸°ê°€ ì„¤ì •ê³¼ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë¦¬ì‚¬ì´ì¦ˆ (ì„ íƒ ì‚¬í•­)
    frame = cv2.resize(frame, (IMG_W, IMG_H))

    # YOLO ì¶”ì 
    results = yolo_model.track(frame, persist=True, verbose=False)

    if results[0].boxes.id is not None:
        boxes = results[0].boxes.xywh.cpu().numpy()
        track_ids = results[0].boxes.id.int().cpu().tolist()
        
        for box, track_id in zip(boxes, track_ids):
            x, y, w, h = box
            center = (float(x), float(y))
            
            # 1. ê¶¤ì  ì—…ë°ì´íŠ¸
            if track_id not in track_history:
                track_history[track_id] = deque(maxlen=SEQ_LENGTH)
            track_history[track_id].append(center)

            # 2. ê³¼ê±° ê¶¤ì  ê·¸ë¦¬ê¸° (íŒŒë€ìƒ‰ ì‹¤ì„ )
            if len(track_history[track_id]) > 1:
                points = np.array(list(track_history[track_id]), dtype=np.int32).reshape((-1, 1, 2))
                cv2.polylines(frame, [points], isClosed=False, color=(255, 0, 0), thickness=2)

            # 3. ë¯¸ë˜ ì˜ˆì¸¡ (ë°ì´í„°ê°€ ì¶©ë¶„íˆ ìŒ“ì˜€ì„ ë•Œ ì‹¤í–‰)
            if len(track_history[track_id]) == SEQ_LENGTH:
                # [ì •ê·œí™”] ì…ë ¥ ì¢Œí‘œë¥¼ 0~1 ì‚¬ì´ë¡œ ë³€í™˜
                input_seq = np.array(list(track_history[track_id]))
                input_seq[:, 0] /= IMG_W
                input_seq[:, 1] /= IMG_H
                input_seq_torch = torch.FloatTensor([input_seq]).to(device)
                
                with torch.no_grad():
                    prediction = lstm_model(input_seq_torch).cpu().numpy()[0]
                
                # [ë³µì›] ê²°ê³¼ ì¢Œí‘œë¥¼ ë‹¤ì‹œ ì˜ìƒ í¬ê¸°ë¡œ ë³€í™˜í•˜ì—¬ ê·¸ë¦¬ê¸°
                for i in range(len(prediction)):
                    # ì˜ˆì¸¡ëœ ì ì€ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ
                    pred_x = int(prediction[i, 0] * IMG_W)
                    pred_y = int(prediction[i, 1] * IMG_H)
                    
                    # í™”ë©´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šì„ ë•Œë§Œ ê·¸ë¦¬ê¸°
                    if 0 <= pred_x < IMG_W and 0 <= pred_y < IMG_H:
                        cv2.circle(frame, (pred_x, pred_y), 5, (0, 0, 255), -1)

    # ê²°ê³¼ í™”ë©´ ì¶œë ¥
    cv2.imshow("A-PAS: Intelligent Trajectory Prediction", frame)
    
    # 'q' ëˆ„ë¥´ë©´ ì¢…ë£Œ
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
print("ğŸ‘‹ ì‹œìŠ¤í…œì´ ì•ˆì „í•˜ê²Œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")