import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import glob
import os

# ==========================================
# âš™ï¸ [ì„¤ì •] í”„ë¡œì íŠ¸ ê·œê²©ì— ë§ê²Œ ê³ ì •
# ==========================================
TRAIN_DATA_DIR = "data/Training"
VAL_DATA_DIR = "data/Validation"
MODEL_SAVE_DIR = "models"
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# í•´ìƒë„ ì„¤ì • (ì •ê·œí™”ì˜ í•µì‹¬)
IMG_W, IMG_H = 1920, 1080 

# í•˜ì´í¼íŒŒë¼ë¯¸í„° (10í”„ë ˆì„ ë°ì´í„°ì…‹ ìµœì í™”)
SEQ_LENGTH = 10     # ê³¼ê±° ê´€ì°° ê¸°ê°„ (0.5ì´ˆ)
PRED_LENGTH = 10    # ë¯¸ë˜ ì˜ˆì¸¡ ê¸°ê°„ (0.3ì´ˆ)
INPUT_SIZE = 2     # x, y ì¢Œí‘œ
HIDDEN_SIZE = 128  
NUM_LAYERS = 2     
BATCH_SIZE = 64
EPOCHS = 50
LEARNING_RATE = 0.001

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… ì¶©ëŒ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (Device: {device})")

# ==========================================
# ğŸ“Š [ë°ì´í„°ì…‹] ì •ê·œí™” ë° ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
# ==========================================
class TrajectoryDataset(Dataset):
    def __init__(self, data_dir):
        self.sequences = []
        self.labels = []
        
        csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
        print(f"ğŸ“‚ {data_dir}ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")

        for file in csv_files:
            df = pd.read_csv(file)
            # ê°ì²´(track_id)ë³„ë¡œ ë¶„ë¦¬í•´ì„œ í•™ìŠµ
            for track_id, group in df.groupby('track_id'):
                # 1. ì¢Œí‘œ ì •ê·œí™” (0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜)
                coords = group[['x_center', 'y_center']].values
                coords[:, 0] /= IMG_W
                coords[:, 1] /= IMG_H
                
                # ìµœì†Œ ê¸¸ì´(SEQ+PRED) ë¯¸ë‹¬ì¸ ì§§ì€ ë°ì´í„°ëŠ” ë²„ë¦¼
                if len(coords) < (SEQ_LENGTH + PRED_LENGTH):
                    continue
                
                # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°: 1í”„ë ˆì„ì”© ë°€ì–´ê°€ë©° ë°ì´í„° ìƒì„±
                for i in range(len(coords) - SEQ_LENGTH - PRED_LENGTH + 1):
                    self.sequences.append(coords[i : i + SEQ_LENGTH])
                    self.labels.append(coords[i + SEQ_LENGTH : i + SEQ_LENGTH + PRED_LENGTH])

        self.sequences = torch.FloatTensor(np.array(self.sequences))
        self.labels = torch.FloatTensor(np.array(self.labels))
        print(f"âœ”ï¸ ì´ {len(self.sequences)}ê°œì˜ í•™ìŠµ ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ!")

    def __len__(self): return len(self.sequences)
    def __getitem__(self, idx): return self.sequences[idx], self.labels[idx]

# ==========================================
# ğŸ§  [ëª¨ë¸] LSTM (Long Short-Term Memory)
# ==========================================
class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        # batch_first=True: (Batch, Seq, Feature) êµ¬ì¡° ì‚¬ìš©
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, PRED_LENGTH * 2)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)
        
        out, _ = self.lstm(x, (h0, c0))
        # ë§ˆì§€ë§‰ ì‹œì ì˜ íŠ¹ì§•ë§Œ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ ì¢Œí‘œ ì˜ˆì¸¡
        out = self.fc(out[:, -1, :]) 
        return out.view(-1, PRED_LENGTH, 2)

# ==========================================
# ğŸš€ [ì‹¤í–‰] í•™ìŠµ ë£¨í”„
# ==========================================
train_dataset = TrajectoryDataset(TRAIN_DATA_DIR)
val_dataset = TrajectoryDataset(VAL_DATA_DIR)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(device)
criterion = nn.MSELoss() 
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

best_val_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0
    for seqs, targets in train_loader:
        seqs, targets = seqs.to(device), targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(seqs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for v_seqs, v_targets in val_loader:
            v_seqs, v_targets = v_seqs.to(device), v_targets.to(device)
            v_outputs = model(v_seqs)
            val_loss += criterion(v_outputs, v_targets).item()
    
    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)
    
    print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")
    
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, "best_trajectory_model.pth"))
        print(f"â­ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸! (Val Loss: {best_val_loss:.6f})")

print("\nğŸ‰ í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ main.pyì—ì„œ ì¶©ëŒì„ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")